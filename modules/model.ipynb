{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11a4bd4",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [],
   "source": [
    "def model(X_train, y_train, params, layers, decay, interval=1000,learning_rate=0.01, decay_rate=0.3, lambda_value=0, batch_size=64,\n",
    "         epoch=100, loops=100, beta1=0.9, beta2=0.999, epsilon=1e-8, threshold=0.5, mini = True, adam_optim = True, testing = False):\n",
    "\n",
    "    m=X_train.shape[0]\n",
    "\n",
    "    all_costs_test = []\n",
    "    all_accs_test = []\n",
    "    \n",
    "    all_costs = []\n",
    "    all_accs = []\n",
    "    \n",
    "    print(f'Model -> hidden layers: {len(layers)-2} | minibatch: {str(mini)} | lambda: {lambda_value} | adam: {str(adam_optim)} | decay: {decay} | lr: {learning_rate}')\n",
    "\n",
    "    if adam_optim == True:\n",
    "        t = 0\n",
    "        v,s = adam_init(params)\n",
    "\n",
    "\n",
    "    # MINIBATCHE\n",
    "    if mini == True:\n",
    "\n",
    "        seed = 0\n",
    "\n",
    "        iterations = math.ceil(m / batch_size) \n",
    "\n",
    "        for i in tqdm(range(epoch), desc='epoch', colour='black'):\n",
    "\n",
    "            \n",
    "            seed += 1\n",
    "            \n",
    "            all_batches = batch_maker(X_train, y_train, batch_size, seed)\n",
    "\n",
    "            for j in range(iterations):\n",
    "                \n",
    "                X_batch, y_batch = all_batches[j]\n",
    "\n",
    "                AL, cache, _ = forward_prop(X_batch, y_batch, params, lambda_value)\n",
    "                \n",
    "                grads = back_prop(AL, X_batch, y_batch, layers, cache, lambda_value)\n",
    "\n",
    "                if adam_optim == True:\n",
    "                    t+=1\n",
    "                    new_params = update_params_adam(params, grads, v, s, t, learning_rate = learning_rate, beta1=beta1, beta2=beta2, epsilon=epsilon)\n",
    "                    params = new_params.copy()\n",
    "\n",
    "                else:\n",
    "                    new_params = update_params(params, grads, learning_rate = learning_rate)\n",
    "                    params = new_params.copy()\n",
    "\n",
    "\n",
    "            if decay == 'exp':\n",
    "                \n",
    "                new_lr = update_lr(learning_rate, i, decay_rate)\n",
    "                learning_rate = new_lr\n",
    "\n",
    "            elif (decay == 'time') & (i%interval==0):\n",
    "                \n",
    "                new_lr = update_lr_time(learning_rate, i, decay_rate, interval)\n",
    "                learning_rate = new_lr\n",
    "                \n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "            _, accuracy, cost = predict(X_train, y_train, params, lambda_value = lambda_value, threshold = threshold)\n",
    "            \n",
    "            all_costs.append(cost) \n",
    "            all_accs.append(accuracy)\n",
    "\n",
    "            if testing == True:\n",
    "\n",
    "                _, acc_test, cost_test = predict(X_test, y_test, params, lambda_value, threshold=threshold)\n",
    "\n",
    "                all_costs_test.append((cost, cost_test))\n",
    "                all_accs_test.append((accuracy, acc_test))\n",
    "                \n",
    "        return all_costs, all_accs, params, AL, all_costs_test, all_accs_test\n",
    "\n",
    "    else: # BATCH \n",
    "\n",
    "        for i in tqdm(range(epoch), desc='iteration', colour='black'):\n",
    "\n",
    "            AL, cache, _ = forward_prop(X_train, y_train, params, lambda_value)\n",
    "\n",
    "            _, acc, cost = predict(X_train, y_train, params, lambda_value, threshold=threshold)\n",
    "            \n",
    "            grads = back_prop(AL, X_train, y_train, layers, cache, lambda_value)\n",
    "            \n",
    "            all_costs.append(cost)\n",
    "            all_accs.append(acc)\n",
    "            \n",
    "            if testing == True:\n",
    "                _, acc_test, cost_test = predict(X_test, y_test, params, lambda_value, threshold=threshold)            \n",
    "                \n",
    "                \n",
    "                all_costs_test.append((cost, cost_test))\n",
    "                all_accs_test.append((acc, acc_test))\n",
    "            \n",
    "            if adam_optim == True:\n",
    "                t += 1\n",
    "                new_params = update_params_adam(params, grads, v, s, t, learning_rate = learning_rate, beta1 = beta1, beta2 = beta2, epsilon = epsilon)\n",
    "                params = new_params.copy()\n",
    "\n",
    "            else:\n",
    "                new_params = update_params(params, grads, learning_rate = learning_rate)\n",
    "                params = new_params.copy()\n",
    "\n",
    "            if decay == 'exp':\n",
    "\n",
    "                new_lr = update_lr_exp(learning_rate, i, decay_rate)\n",
    "                learning_rate = new_lr\n",
    "\n",
    "            elif (decay == 'time') & (i % interval==0):\n",
    "\n",
    "                new_lr = update_lr_time(learning_rate, i, decay_rate, interval)\n",
    "                learning_rate = new_lr\n",
    "                \n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "        return all_costs, all_accs, params, AL, all_costs_test, all_accs_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
