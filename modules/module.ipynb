{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2baad0d3",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": null,
     "id": "c6254c96",
     "kernelId": "39b651f9-124b-441e-9245-afd31c89c74b"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import re\n",
    "import csv\n",
    "import math\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import loguniform\n",
    "from scipy.special import xlogy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from mpl_toolkits.axes_grid1.inset_locator import mark_inset\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "import matplotlib.gridspec as gridspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8d0c5e",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": null,
     "id": "d3f2f108",
     "kernelId": "39b651f9-124b-441e-9245-afd31c89c74b"
    }
   },
   "outputs": [],
   "source": [
    "# Plot settings\n",
    "mpl.rcParams['axes.spines.left'] = True\n",
    "mpl.rcParams['axes.spines.right'] = False\n",
    "mpl.rcParams['axes.spines.top'] = False\n",
    "mpl.rcParams['axes.spines.bottom'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1bd5f9",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": null,
     "id": "416e0a40",
     "kernelId": "39b651f9-124b-441e-9245-afd31c89c74b"
    }
   },
   "outputs": [],
   "source": [
    "def new_test(X, grad_check=False):\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    \n",
    "    if grad_check == True:\n",
    "        layers = [X.shape[1], 2, 1]\n",
    "        params = parameter_init(layers, zero=False, he=True)\n",
    "        \n",
    "    else: \n",
    "        layers = [X.shape[1], 500, 500, 1]\n",
    "        params = parameter_init(layers, zero=False, he=True)\n",
    "        \n",
    "    return params, layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb3d14f",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "91902179",
     "kernelId": "39b651f9-124b-441e-9245-afd31c89c74b"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return (1/(1 + np.exp(-z)))\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "def softmax(z):\n",
    "    expo = np.exp(z)\n",
    "    expo_sum = np.sum(np.exp(z))\n",
    "    return expo/expo_sum\n",
    "def inv_relu(z):\n",
    "    return (z > 0) * 1\n",
    "def inv_sigmoid(z):\n",
    "    return np.multiply(sigmoid(z), 1 - sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68f96de",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "e2191006",
     "kernelId": "39b651f9-124b-441e-9245-afd31c89c74b"
    }
   },
   "outputs": [],
   "source": [
    "def encoder(y, binary=True):\n",
    "\n",
    "    if binary == True:\n",
    "        encoder = OneHotEncoder(drop='first', handle_unknown='error')\n",
    "        \n",
    "    else:\n",
    "        encoder = OneHotEncoder(handle_unknown='error')\n",
    "\n",
    "    y_enc = encoder.fit_transform(np.array(y).reshape(y.shape[0], 1)).toarray()\n",
    "\n",
    "    return y_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6edaef1",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "fe8eba32",
     "kernelId": "39b651f9-124b-441e-9245-afd31c89c74b"
    }
   },
   "outputs": [],
   "source": [
    "def parameter_init(layers, zero=False, he=False):\n",
    "    \n",
    "    params={}\n",
    "\n",
    "    for i in range(len(layers)-1):\n",
    "        \n",
    "        if zero == True:\n",
    "            params['w' + str(i + 1)] = np.zeros((layers[i], layers[i + 1]))\n",
    "            params['b' + str(i + 1)] = np.zeros((1, layers[i + 1]))\n",
    "\n",
    "        else:\n",
    "            params['w' + str(i + 1)] = np.random.randn(layers[i], layers[i + 1]) *0.01\n",
    "            params['b' + str(i + 1)] = np.random.randn(1, layers[i + 1])\n",
    "\n",
    "            if he == True:\n",
    "                params['w' + str(i + 1)] *= np.sqrt(2/layers[i])\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066ba081",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": null,
     "id": "b66f9a68",
     "kernelId": "39b651f9-124b-441e-9245-afd31c89c74b"
    }
   },
   "outputs": [],
   "source": [
    "def dict_to_vec(dictionary):\n",
    "\n",
    "    dict_vector = np.empty([0, 1])\n",
    "    shapes = []\n",
    "\n",
    "    regex_w = 'd{0,1}w'\n",
    "    regex_b = 'd{0,1}b'\n",
    "\n",
    "    ws = [v for k, v in dictionary.items() if re.match(regex_w, k)]\n",
    "    bs = [v for k, v in dictionary.items() if re.match(regex_b, k)]\n",
    "\n",
    "    for i in range(int(len(dictionary)/2)):\n",
    "        w_shape = ws[i].shape\n",
    "        b_shape = bs[i].shape\n",
    "\n",
    "        w_vec = ws[i].flatten('F').reshape(-1,1)\n",
    "        b_vec = bs[i].flatten('F').reshape(-1,1)\n",
    "\n",
    "        dict_vector = np.concatenate([dict_vector, w_vec, b_vec])\n",
    "\n",
    "        shapes.append((w_shape, b_shape))\n",
    "\n",
    "    return dict_vector, shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fcdc23",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "d39bc9fa",
     "kernelId": "39b651f9-124b-441e-9245-afd31c89c74b"
    }
   },
   "outputs": [],
   "source": [
    "def vec_to_dict(vector, param_shapes, params):\n",
    "    \n",
    "    new_dict ={}\n",
    "    \n",
    "    dictionary = dict.copy(params)\n",
    "\n",
    "    for i in range(1, int(len(params) / 2 + 1)):\n",
    "        w_shape = param_shapes[i-1][0]\n",
    "        w_count = w_shape[0] * w_shape[1]\n",
    "\n",
    "        b_shape = param_shapes[i-1][1]\n",
    "        b_count = b_shape[0] * b_shape[1]\n",
    "\n",
    "        regex_w = f'd*w{i}$'\n",
    "        regex_b = f'd*b{i}$'\n",
    "\n",
    "        w_i = [k for k in dictionary.keys() if re.match(regex_w, k)][0]\n",
    "        b_i = [k for k in dictionary.keys() if re.match(regex_b, k)][0]\n",
    "\n",
    "        new_dict[w_i] = vector[0: w_count].reshape(w_shape, order = 'F')\n",
    "\n",
    "        vector = vector[w_count:]\n",
    "\n",
    "        new_dict[b_i] = vector[: b_count].reshape(b_shape, order = 'F')\n",
    "\n",
    "        vector = vector[b_count:]\n",
    "\n",
    "    return new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5b4308",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "57048c00",
     "kernelId": "39b651f9-124b-441e-9245-afd31c89c74b"
    }
   },
   "outputs": [],
   "source": [
    "def cost_func(A, y, lambda_value=0.1, regularization=False):\n",
    "    \n",
    "    reg_sums = 0\n",
    "    m=y.shape[0]\n",
    "\n",
    "    cost = (-1 / m) * np.sum(np.multiply(np.log(A), y) + np.multiply(np.log(1 - A), (1 - y)))\n",
    "\n",
    "    if regularization == True:\n",
    "        \n",
    "        for i in range(1, int(len(params)/2+1)):\n",
    "            reg_sums += np.sum(params['w' + str(i)]**2)\n",
    "\n",
    "        l2_reg = cost + (lambda_value/(2*m)) * reg_sums\n",
    "        \n",
    "        return l2_reg\n",
    "\n",
    "    else:\n",
    "        return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f9511d",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "d0b22233",
     "kernelId": "39b651f9-124b-441e-9245-afd31c89c74b"
    }
   },
   "outputs": [],
   "source": [
    "# def cost_func(A, y, lambda_value=0.1):\n",
    "#     reg_sums = 0\n",
    "#     m=y.shape[0]\n",
    "\n",
    "#     cost = (-1 / m) * np.sum(np.multiply(np.log(A), y) + np.multiply(np.log(1 - A), (1 - y)))\n",
    "    \n",
    "#     for i in range(1, int(len(params)/2+1)):\n",
    "#         reg_sums += np.sum(params['w' + str(i)]**2)\n",
    "\n",
    "#     cost += (lambda_value/(2*m)) * reg_sums\n",
    "        \n",
    "\n",
    "#     return l2_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6267ec9f",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "2a5eb997",
     "kernelId": "39b651f9-124b-441e-9245-afd31c89c74b"
    }
   },
   "outputs": [],
   "source": [
    "def forward_prop_wrong(X, params, sig=True):\n",
    "    \n",
    "    cache=[]\n",
    "\n",
    "    last = str(len(layers)-1)\n",
    "\n",
    "    # First Layer\n",
    "    w = params['w1']\n",
    "    b = params['b1']\n",
    "    z = np.matmul(X, w) + b\n",
    "\n",
    "    A = relu(z)\n",
    "\n",
    "    l = (w, b, z, A)\n",
    "\n",
    "    cache.append(l)\n",
    "\n",
    "    if len(layers) > 3: # todo this didn't seem to make a difference but still (but consider removing)\n",
    "\n",
    "        for i in range(2,len(layers)-1):\n",
    "            w = params['w' + str(i)]\n",
    "            b = params['b' + str(i)]\n",
    "            z = np.matmul(A, w) + b\n",
    "\n",
    "            A = relu(z)\n",
    "\n",
    "            l = (w, b, z, A)\n",
    "            cache.append(l)\n",
    "\n",
    "\n",
    "    # Last layer\n",
    "    w = params['w' + last]\n",
    "    b = params['b' + last]\n",
    "    z = np.matmul(A, w) + b\n",
    "\n",
    "    if sig == True:\n",
    "        A = sigmoid(z)\n",
    "        \n",
    "    else:\n",
    "        A = softmax(z)\n",
    "\n",
    "    l = (w, b, z, A)\n",
    "\n",
    "    cache.append(l)\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadcd36a",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "ec61b4da",
     "kernelId": "39b651f9-124b-441e-9245-afd31c89c74b"
    }
   },
   "outputs": [],
   "source": [
    "def forward_prop(X, y, params, lambda_value):\n",
    "    \n",
    "    cache=[]\n",
    "\n",
    "    last = str(int(len(params)/2))\n",
    "    \n",
    "    m = X.shape[0]\n",
    "    \n",
    "    # First and mid layers\n",
    "    for i in range(1,int(len(params)/2)):\n",
    "\n",
    "        if i == 1:\n",
    "            A = X.copy()\n",
    "\n",
    "        A_prev = A.copy()\n",
    "\n",
    "        w = params['w' + str(i)]\n",
    "        b = params['b' + str(i)]\n",
    "\n",
    "        z = np.matmul(A_prev, w) + b\n",
    "\n",
    "        A = relu(z)\n",
    "\n",
    "        l = (w, b, z, A)\n",
    "        \n",
    "        cache.append(l)\n",
    "\n",
    "    # Last layer\n",
    "    w = params['w' + last]\n",
    "    b = params['b' + last]\n",
    "\n",
    "    z = np.matmul(A, w) + b\n",
    "\n",
    "    AL = sigmoid(z)\n",
    "\n",
    "    l = (w, b, z, AL)\n",
    "\n",
    "    cache.append(l)\n",
    "\n",
    "    reg_sums = 0\n",
    "    m=y.shape[0]\n",
    "    \n",
    "    cost_nonreg = (-1 / m) * np.sum(xlogy(y, AL) + xlogy(1 - y, 1 - AL))\n",
    "    # compensating for times when AL = 0 or 1\n",
    "\n",
    "    for i in range(1, int(len(params) / 2 + 1)):\n",
    "        reg_sums += np.sum(params['w' + str(i)] ** 2)\n",
    "\n",
    "    l2_reg = (lambda_value / (2 * m)) * reg_sums\n",
    "\n",
    "    cost = cost_nonreg + l2_reg\n",
    "\n",
    "    return AL, cache, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644340a0",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "2d2b02fb",
     "kernelId": "39b651f9-124b-441e-9245-afd31c89c74b"
    }
   },
   "outputs": [],
   "source": [
    "def back_prop_wrong(A, X, y, layers, cache, lambda_value=0.1, regularization=False):\n",
    "    grads = {}\n",
    "    cache_dict = {}\n",
    "    m = y.shape[0]\n",
    "    last = len(layers) - 1\n",
    "\n",
    "    if regularization == True:\n",
    "        switch = 1\n",
    "    else:\n",
    "        switch = 0\n",
    "\n",
    "    for i in range(len(cache)):\n",
    "        cache_dict['w' + str(i + 1)] = np.copy(cache[i][0])\n",
    "        cache_dict['b' + str(i + 1)] = np.copy(cache[i][1])\n",
    "        cache_dict['z' + str(i + 1)] = np.copy(cache[i][2])\n",
    "        cache_dict['a' + str(i + 1)] = np.copy(cache[i][3])\n",
    "\n",
    "    # Back prop for very last layer\n",
    "    grads['dz' + str(last)] = A - y \n",
    "\n",
    "    grads['db' + str(last)] = (1 / m) * np.sum(grads['dz' + str(last)], axis=0, keepdims=True)\n",
    "    grads['dw' + str(last)] = (1 / m) * np.matmul(cache_dict['a' + str(last - 1)].T, grads['dz' + str(last)]) + (\n",
    "                (lambda_value / m) * cache_dict['w' + str(last)]) * switch\n",
    "\n",
    "\n",
    "    # Back prop for middle layers\n",
    "    for i in reversed(range(2, last)):\n",
    "        grads['dz' + str(i)] = np.multiply(\n",
    "            np.matmul(grads['dz' + str(i + 1)], cache_dict['w' + str(i + 1)].T),\n",
    "            inv_relu(cache_dict['z' + str(i)]))\n",
    "        grads['db' + str(i)] = (1 / m) * np.sum(grads['dz' + str(i)], axis=0, keepdims=True)\n",
    "        grads['dw' + str(i)] = (1 / m) * np.matmul(cache_dict['a' + str(i - 1)].T, grads['dz' + str(i)]) + (\n",
    "                    (lambda_value / m) * cache_dict['w' + str(i)]) * switch\n",
    "\n",
    "    # Back prop for first layer\n",
    "    grads['dz1'] = np.multiply(\n",
    "        np.matmul(grads['dz2'], cache_dict['w2'].T),\n",
    "        inv_relu(cache_dict['z1']))\n",
    "    grads['db1'] = (1 / m) * np.sum(grads['dz1'], axis=0, keepdims=True)\n",
    "    grads['dw1'] = (1 / m) * np.matmul(X.T, grads['dz1']) + ((lambda_value / m) * cache_dict['w1']) * switch\n",
    "\n",
    "    for i in range(1, len(layers)):\n",
    "        del (grads['dz' + str(i)])\n",
    "\n",
    "    grads_new = {}\n",
    "    for key in reversed(grads):\n",
    "        grads_new[key] = grads[key]\n",
    "\n",
    "    return grads_new "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566e3902",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "a2636167",
     "kernelId": "39b651f9-124b-441e-9245-afd31c89c74b"
    }
   },
   "outputs": [],
   "source": [
    "def back_prop(AL, X, y, layers, cache, lambda_value): \n",
    "    \n",
    "    grads = {}\n",
    "    cache_dict = {}\n",
    "    m = y.shape[0]\n",
    "    last = len(layers) - 1\n",
    "\n",
    "    for i in range(len(cache)):     \n",
    "        cache_dict['w' + str(i + 1)] = np.copy(cache[i][0])\n",
    "        cache_dict['b' + str(i + 1)] = np.copy(cache[i][1])\n",
    "        cache_dict['z' + str(i + 1)] = np.copy(cache[i][2])\n",
    "        cache_dict['a' + str(i + 1)] = np.copy(cache[i][3])\n",
    "\n",
    "    # Back prop for last layer\n",
    "    grads['dz' + str(last)] = AL - y\n",
    "    grads['db' + str(last)] = (1 / m) * np.sum(grads['dz' + str(last)], axis=0, keepdims=True)\n",
    "    grads['dw' + str(last)] = (1 / m) * np.matmul(cache_dict['a' + str(last - 1)].T, grads['dz' + str(last)]) + ((lambda_value / m) * cache_dict['w' + str(last)])\n",
    "\n",
    "    # Back prop for middle layers\n",
    "    for i in reversed(range(2, last)):   \n",
    "        grads['dz' + str(i)] = np.multiply(np.matmul(grads['dz' + str(i + 1)], cache_dict['w' + str(i + 1)].T), inv_relu(cache_dict['z' + str(i)]))\n",
    "        grads['db' + str(i)] = (1 / m) * np.sum(grads['dz' + str(i)], axis=0, keepdims=True)\n",
    "        grads['dw' + str(i)] = (1 / m) * np.matmul(cache_dict['a' + str(i - 1)].T, grads['dz' + str(i)]) + ((lambda_value / m) * cache_dict['w' + str(i)])\n",
    "\n",
    "    # Back prop for first layer\n",
    "    grads['dz1'] = np.multiply(np.matmul(grads['dz2'], cache_dict['w2'].T),inv_relu(cache_dict['z1']))\n",
    "    grads['db1'] = (1 / m) * np.sum(grads['dz1'], axis=0, keepdims=True)\n",
    "    grads['dw1'] = (1 / m) * np.matmul(X.T, grads['dz1']) + ((lambda_value / m) * cache_dict['w1'])\n",
    "\n",
    "    for i in range(1, len(layers)):\n",
    "        del (grads['dz' + str(i)])\n",
    "\n",
    "    grads_new = {}\n",
    "    \n",
    "    for key in reversed(grads):\n",
    "        grads_new[key] = grads[key]\n",
    "\n",
    "    return grads_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2aef1a9",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "b335a905",
     "kernelId": "39b651f9-124b-441e-9245-afd31c89c74b"
    }
   },
   "outputs": [],
   "source": [
    "def grad_check_wrong(X, A, y, params, grads, epsilon=1e-7):\n",
    "\n",
    "    temp_params = params.copy()  \n",
    "\n",
    "    temp_vector, shapes = dict_to_vec(temp_params)\n",
    "\n",
    "    cost_plus = np.zeros(temp_vector.shape)\n",
    "    cost_minus = np.zeros(temp_vector.shape)\n",
    "\n",
    "    grads_approx = np.zeros(temp_vector.shape)\n",
    "\n",
    "    grads_vector, _ = dict_to_vec(grads)\n",
    "    grads_vector\n",
    "\n",
    "    for i in range(len(temp_vector)):\n",
    "        temp_vector_plus = np.copy(temp_vector)\n",
    "        temp_vector_minus = np.copy(temp_vector)\n",
    "\n",
    "        temp_vector_plus[i] += epsilon\n",
    "        temp_vector_minus[i] -= epsilon\n",
    "\n",
    "        temp_dict_plus = vec_to_dict(temp_vector_plus, shapes, params)\n",
    "        temp_dict_minus = vec_to_dict(temp_vector_minus, shapes, params)\n",
    "\n",
    "        A_plus, _ = forward_prop_wrong(X, temp_dict_plus, sig=True)\n",
    "        A_minus, _ = forward_prop_wrong(X, temp_dict_minus, sig=True)\n",
    "\n",
    "        cost_plus[i] = cost_func(A_plus, y, lambda_value =0.1)\n",
    "\n",
    "        cost_minus[i] = cost_func(A_minus, y, lambda_value =0.1)\n",
    "\n",
    "        grads_approx[i] = (cost_plus[i] - cost_minus[i]) / (2 * epsilon)\n",
    "\n",
    "    num = np.linalg.norm(grads_approx - grads_vector)\n",
    "    den = np.linalg.norm(grads_approx) + np.linalg.norm(grads_vector)\n",
    "    diff = num / den\n",
    "\n",
    "    if diff > epsilon:\n",
    "        print('Something might be wrong!')\n",
    "        \n",
    "    else:\n",
    "        print('All good!')\n",
    "\n",
    "    return diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127e5292",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": null,
     "id": "dfc8c869",
     "kernelId": "39b651f9-124b-441e-9245-afd31c89c74b"
    }
   },
   "outputs": [],
   "source": [
    "def grad_check(X, y, params, grads, lambda_value, epsilon=1e-7):\n",
    "    \n",
    "    temp_params = params.copy()\n",
    "\n",
    "    params_vector, param_shapes = dict_to_vec(temp_params)\n",
    "\n",
    "    grads_approx = np.zeros(params_vector.shape)\n",
    "\n",
    "    grads_vector, _ = dict_to_vec(grads)\n",
    "    \n",
    "    cost_plus = np.zeros(params_vector.shape)\n",
    "    cost_minus = np.zeros(params_vector.shape)\n",
    "\n",
    "    for i in tqdm(range(len(params_vector)), desc='grad check', colour='black'):\n",
    "        params_plus = np.copy(params_vector)\n",
    "        params_minus = np.copy(params_vector)\n",
    "\n",
    "        params_plus[i] += epsilon\n",
    "        params_minus[i] -= epsilon\n",
    "\n",
    "        params_dict_plus = vec_to_dict(params_plus, param_shapes, params)\n",
    "        _,_, cost_plus[i] = forward_prop(X,y, params_dict_plus, lambda_value)\n",
    "\n",
    "        params_dict_minus = vec_to_dict(params_minus, param_shapes, params)\n",
    "        _, _, cost_minus[i] = forward_prop(X,y, params_dict_minus, lambda_value)\n",
    "\n",
    "        grads_approx[i] = (cost_plus[i] - cost_minus[i]) / (2 * epsilon)\n",
    "\n",
    "    num = np.linalg.norm(grads_approx - grads_vector)\n",
    "    den = np.linalg.norm(grads_approx) + np.linalg.norm(grads_vector)\n",
    "    diff = num / den\n",
    "\n",
    "    if diff > epsilon:\n",
    "        print('Something might be wrong!')\n",
    "        \n",
    "    else:\n",
    "        print(f'All good! Difference is = {diff}')\n",
    "        \n",
    "    return diff, num, den"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcd4e45",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "793e4e7d",
     "kernelId": "39b651f9-124b-441e-9245-afd31c89c74b"
    }
   },
   "outputs": [],
   "source": [
    "def update_params(params, grads, learning_rate=0.01):\n",
    "\n",
    "    new_params = {}\n",
    "\n",
    "    for i in range(1, int(len(params)/2 + 1)):\n",
    "        new_params['w' + str(i)] = params['w' + str(i)] - learning_rate * grads['dw' + str(i)]\n",
    "        new_params['b' + str(i)] = params['b' + str(i)] - learning_rate * grads['db' + str(i)]\n",
    "\n",
    "    return new_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d273d3",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "da2a8684",
     "kernelId": "39b651f9-124b-441e-9245-afd31c89c74b"
    }
   },
   "outputs": [],
   "source": [
    "def predict(X, y, parameters, lambda_value, threshold=0.5):\n",
    "    \n",
    "    A, _, cost = forward_prop(X, y, parameters, lambda_value)\n",
    "    \n",
    "    #https://stackoverflow.com/questions/56321906/make-all-the-elemant-zero-except-max-n-element-in-each-row-in-numpy-2d-array\n",
    "    n_max = 1\n",
    "    \n",
    "    A = A * (A >= np.sort(A, axis=1)[:, [-n_max]]).astype(int)\n",
    "    \n",
    "    outcome = (A>threshold)*1\n",
    "    accuracy = np.sum(y == outcome)/(len(y))\n",
    "\n",
    "    return outcome, accuracy, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409875a4",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "0afc272c",
     "kernelId": "39b651f9-124b-441e-9245-afd31c89c74b"
    }
   },
   "outputs": [],
   "source": [
    "def f_score(outcome, y):\n",
    "\n",
    "    # false neg\n",
    "    false_neg = ((y == 1) & (outcome == 0)).sum()\n",
    "    # false pos\n",
    "    false_pos = ((y == 0) & (outcome == 1)).sum()\n",
    "    # true pos\n",
    "    true_pos = ((y == 1) & (outcome == 1)).sum()\n",
    "\n",
    "    precision = true_pos / (true_pos + false_pos)\n",
    "    recall = true_pos / (true_pos + false_neg)\n",
    "\n",
    "    score = 2 * ((precision * recall) / (precision + recall))\n",
    "\n",
    "    return score, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d04653",
   "metadata": {
    "gradient": {
     "id": "bc77aca1",
     "kernelId": "39b651f9-124b-441e-9245-afd31c89c74b"
    }
   },
   "outputs": [],
   "source": [
    "def adam_init(params):\n",
    "    \n",
    "    v = {}\n",
    "    s = {}\n",
    "\n",
    "    for i in range(1, int(len(params)/2)+1):\n",
    "        v['dw' + str(i)] = np.zeros((params[\"w\" + str(i)].shape[0], params[\"w\" + str(i)].shape[1]))\n",
    "        v['db' + str(i)] = np.zeros((params[\"b\" + str(i)].shape[0], params[\"b\" + str(i)].shape[1]))\n",
    "\n",
    "        s['dw' + str(i)] = np.zeros((params[\"w\" + str(i)].shape[0], params[\"w\" + str(i)].shape[1]))\n",
    "        s['db' + str(i)] = np.zeros((params[\"b\" + str(i)].shape[0], params[\"b\" + str(i)].shape[1]))\n",
    "\n",
    "    return v,s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151f2cd1",
   "metadata": {
    "gradient": {
     "id": "35f7124d",
     "kernelId": "39b651f9-124b-441e-9245-afd31c89c74b"
    }
   },
   "outputs": [],
   "source": [
    "def update_params_adam(params, grads, v, s, t, learning_rate=0.01, beta1 = 0.9, beta2 = 0.999, epsilon = 1e-8):\n",
    "    \n",
    "    v_adjusted = {}\n",
    "    s_adjusted = {}\n",
    "\n",
    "    for i in range(1, int(len(params)/2)+1):\n",
    "        v['dw' + str(i)] = beta1 * v['dw' + str(i)] + (1 - beta1) * grads['dw' + str(i)]\n",
    "        v['db' + str(i)] = beta1 * v['db' + str(i)] + (1 - beta1) * grads['db' + str(i)]\n",
    "\n",
    "        v_adjusted[\"dw\" + str(i)] = v['dw' + str(i)] / (1 - np.power(beta1, t))\n",
    "        v_adjusted[\"db\" + str(i)] = v['db' + str(i)] / (1 - np.power(beta1, t))\n",
    "\n",
    "        s[\"dw\" + str(i)] = beta2 * s['dw' + str(i)] + (1 - beta2) * np.power(grads['dw' + str(i)], 2)\n",
    "        s[\"db\" + str(i)] = beta2 * s['db' + str(i)] + (1 - beta2) * np.power(grads['db' + str(i)], 2)\n",
    "\n",
    "        s_adjusted[\"dw\" + str(i)] = s[\"dw\" + str(i)] / (1 - np.power(beta2, t))\n",
    "        s_adjusted[\"db\" + str(i)] = s[\"db\" + str(i)] / (1 - np.power(beta2, t))\n",
    "\n",
    "        params[\"w\" + str(i)] = params[\"w\" + str(i)] - (\n",
    "                    learning_rate * v_adjusted[\"dw\" + str(i)] / (np.sqrt(s_adjusted[\"dw\" + str(i)]) + epsilon))\n",
    "        params[\"b\" + str(i)] = params[\"b\" + str(i)] - (\n",
    "                    learning_rate * v_adjusted[\"db\" + str(i)] / (np.sqrt(s_adjusted[\"db\" + str(i)]) + epsilon))\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd0b9a1",
   "metadata": {
    "gradient": {
     "id": "d90c70cd",
     "kernelId": "39b651f9-124b-441e-9245-afd31c89c74b"
    }
   },
   "outputs": [],
   "source": [
    "def batch_maker(X, y, batch_size, seed):\n",
    "\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    m = X.shape[0]\n",
    "    perm = np.random.permutation(m)\n",
    "\n",
    "    X_shuffled = X[perm, :]\n",
    "    y_shuffled = y[perm, :]\n",
    "\n",
    "    all_batches = []\n",
    "\n",
    "    for i in range(0, math.floor(m / batch_size)):\n",
    "        batch_X = X_shuffled[i * batch_size: (i + 1) * batch_size, :]\n",
    "        batch_y = y_shuffled[i * batch_size: (i + 1) * batch_size, :]\n",
    "\n",
    "        batch_pair = (batch_X, batch_y)\n",
    "\n",
    "        all_batches.append(batch_pair)\n",
    "\n",
    "    if m % batch_size != 0:\n",
    "        batch_X = X_shuffled[math.floor(m / batch_size) * batch_size:, :]\n",
    "        batch_y = y_shuffled[math.floor(m / batch_size) * batch_size:, :]\n",
    "\n",
    "        batch_pair = (batch_X, batch_y)\n",
    "\n",
    "        all_batches.append(batch_pair)\n",
    "\n",
    "    return all_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46c5866",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "93517202",
     "kernelId": "39b651f9-124b-441e-9245-afd31c89c74b"
    }
   },
   "outputs": [],
   "source": [
    "def update_lr_exp(learning_rate, epoch_num, decay_rate):\n",
    "\n",
    "    new_learning_rate = (1 / (1 + decay_rate * epoch_num)) * learning_rate\n",
    "\n",
    "    return new_learning_rate\n",
    "\n",
    "\n",
    "def update_lr_time(learning_rate, epoch_num, decay_rate, interval):\n",
    "\n",
    "    new_learning_rate = (1 / (1 + decay_rate * math.floor(epoch_num / interval))) * learning_rate\n",
    "\n",
    "    return new_learning_rate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
